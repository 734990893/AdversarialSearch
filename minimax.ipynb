{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search Algorithm in Game Theory #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mimimax Algorithm ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Prerequest_: ####\n",
    "1. Multiplayers zero-sum (one player wins and others lose) adversarial game\n",
    "2. Players take turns to make move\n",
    "3. Assume opponent plays optimal way\n",
    "4. Need a score(value) associated with each state (usaully from evaluation funciton)\n",
    "5. Need a winning and losing state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Description_: ####\n",
    "- Each possible move will generate a successor state, and player will choose the move that will lead to the state with highest score.\n",
    "- There will be two kinds of layers, MAX layer and MIN layer\n",
    "- MAX layer gets the maximum value of its children\n",
    "- MIN layer gets the minimum value of its children\n",
    "- MAX layer and MIN layer happening iteratively and repeately, +1 level for each layer and +1 depth for each pair,until the game tree reaches a terminal state or the maximum depth defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Example_: ####\n",
    "![image info](./minimax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Complexity_: ####\n",
    "    _Assmue each node has b successors and depth is d_\n",
    "    \n",
    "    O(bˣbˣbˣ...ˣb) = O(b^d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxPolicy():\n",
    "    def __init__(self, index=0, depth=2):\n",
    "        \"\"\"Abstract class of Algorithms using Minimax Policy\n",
    "        By default 3 kinds of optimizer defined\n",
    "        1. Minimizer, returns the tupe with minimum first value\n",
    "        2. Maximizer, returns the tupe with maximum first value\n",
    "        3. Expectation_Adder, returns a tuple containing sum of first values and None\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Current agent index in agent array.\n",
    "        depth : int\n",
    "            The depth of game tree going to expand.\n",
    "        \"\"\"\n",
    "        self.index = 0\n",
    "        self.depth = depth\n",
    "        self.minimizer = lambda *iterable: min(iterable,\n",
    "                                               key=lambda val: val[0])\n",
    "        self.maximizer = lambda *iterable: max(iterable,\n",
    "                                               key=lambda val: val[0])\n",
    "        self.expectation_adder = lambda *iterable: (sum([val[0]\n",
    "                                                         for val\n",
    "                                                         in iterable]),\n",
    "                                                    None)\n",
    "\n",
    "    def evaluationFunction(self, game_state):\n",
    "        \"\"\"\n",
    "        @todo To be implemented according to the rule of game\n",
    "        \"\"\" \n",
    "        raise NotImplementedError(\"To be implemented\")\n",
    "        \n",
    "    def get_optimize_specifics(self, agent_index):\n",
    "        \"\"\"\n",
    "        Get optimizer and inital best score\n",
    "        Abstract function to be defined in inheritor separately.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int\n",
    "            The agent index in agent array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (function, (float, Action))\n",
    "            tuple of optimizer and inital best score from agent index\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented\")\n",
    "\n",
    "    def minimax(self, agent_index, game_state, depth, alpha=None, beta=None):\n",
    "        \"\"\"\n",
    "        Get optimizer and inital best score\n",
    "        Abstract function to be defined in inheritor separately.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int\n",
    "            The agent index in agent array.\n",
    "        game_state: State\n",
    "            State of the game\n",
    "        depth: int\n",
    "            Current depth\n",
    "        alpha: int\n",
    "            Alpha value if using alpha-beta pruning\n",
    "        beta: int\n",
    "            Beta value if using alpha-beta purning\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (function, (float, Action))\n",
    "            tuple of optimizer and inital best score from agent index\n",
    "        \"\"\"\n",
    "        # Check Terminal State or not\n",
    "        if game_state.isWin() or game_state.isLose():\n",
    "            return (self.evaluationFunction(game_state), None)\n",
    "\n",
    "        optimizer, best_score = self.get_optimize_specifics(agent_index)\n",
    "\n",
    "        # Take one step ahead\n",
    "        if agent_index + 1 < game_state.getNumAgents():\n",
    "            next_agent_index = agent_index + 1\n",
    "            next_depth = depth\n",
    "        else:\n",
    "            next_agent_index = 0\n",
    "            next_depth = depth + 1\n",
    "\n",
    "        # Traverse through possible successors\n",
    "        legal_actions = game_state.getLegalActions(agent_index)\n",
    "        for action in legal_actions:\n",
    "            successor_state = game_state.generateSuccessor(agent_index,\n",
    "                                                           action)\n",
    "            # Get score of current node if reaches the max depth\n",
    "            # otherwise keep expanding\n",
    "            if next_depth > self.depth:\n",
    "                successor_score = (self.evaluationFunction(successor_state),\n",
    "                                   None)\n",
    "            else:\n",
    "                successor_score = (self.minimax(next_agent_index,\n",
    "                                                successor_state,\n",
    "                                                next_depth,\n",
    "                                                alpha,\n",
    "                                                beta)[0],\n",
    "                                   action)\n",
    "\n",
    "            # Update Best score and alpha beta values if applies\n",
    "            if optimizer == self.maximizer:\n",
    "                best_score = optimizer(best_score,\n",
    "                                       successor_score)\n",
    "                alpha = alpha and optimizer(alpha,\n",
    "                                            best_score)\n",
    "            elif optimizer == self.minimizer:\n",
    "                best_score = optimizer(best_score,\n",
    "                                       successor_score)\n",
    "                beta = beta and optimizer(beta,\n",
    "                                          best_score)\n",
    "            elif optimizer is self.expectation_adder:\n",
    "                best_score = optimizer(best_score,\n",
    "                                       (1./len(legal_actions) *\n",
    "                                        successor_score[0],\n",
    "                                        None))\n",
    "            else:\n",
    "                 raise NotImplementedError(\"To be implemented\")\n",
    "\n",
    "            # Pruning if applies\n",
    "            if alpha and beta and alpha[0] >= beta[0]:\n",
    "                return best_score\n",
    "\n",
    "        return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxAgent(MinimaxPolicy):\n",
    "    def __init__(self, index, depth):\n",
    "        \"\"\"Agent using minimax algorithm\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Current agent index in agent array.\n",
    "        depth : int\n",
    "            The depth of game tree going to expand.\n",
    "        \"\"\"\n",
    "        self._player_optimizer = (self.maximizer, (-float('inf'), None))\n",
    "        self._opponent_optimizer = (self.minimizer, (float('inf'), None))\n",
    "        \n",
    "        return super().__init__(index=index, depth=depth)\n",
    "    \n",
    "    def evaluationFunction(self, game_state):\n",
    "        \"\"\"        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_state : State\n",
    "            Game State.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Value associated with the game state\n",
    "        \"\"\" \n",
    "        game_state.get_score()\n",
    "    \n",
    "    def get_optimize_specifics(self, agent_index):\n",
    "        \"\"\"\n",
    "        Get optimizer and inital best score\n",
    "        \"\"\"\n",
    "        if agent_index == self.index:\n",
    "            return (self.maximizer, (-float('inf'), None))\n",
    "        else:\n",
    "            return (self.minimizer, (float('inf'), None))\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        \"\"\"\n",
    "        Returns the action associated with best score\n",
    "        \"\"\"\n",
    "        _, action = self.minimax(self.index,\n",
    "                                 gameState,\n",
    "                                 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Beta Pruning (Optimation Method of Minimax) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Desciption_: ####\n",
    "    - There will be two variables storing evaluated max and min values, which are ⍺ and β respectively\n",
    "    - Initial value of ⍺ is -∞, and initial value of β is ∞\n",
    "    - MAX layer only update ⍺\n",
    "    - MIN layer only update β\n",
    "    - Pruning the rest whenever ⍺ >= β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Example_: ####\n",
    "![image info](./alpha-beta.jpg)\n",
    "```\n",
    "Step 1:\n",
    "MAX{                  ⍺ = -∞\n",
    "        β = ∞\n",
    "        ↓\n",
    "    MIN{3, 5, 10},\n",
    "    MIN{2, a, b},\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 2:\n",
    "MAX{                  ⍺ = -∞\n",
    "           β = 3\n",
    "           ↓\n",
    "    MIN{3, 5, 10},\n",
    "    MIN{2, a, b},\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 3:\n",
    "MAX{                  ⍺ = -∞\n",
    "              β = 3\n",
    "              ↓\n",
    "    MIN{3, 5, 10},\n",
    "    MIN{2, a, b},\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 4:\n",
    "MAX{                  ⍺ = 3\n",
    "    MIN{3, 5, 10},\n",
    "        β = ∞\n",
    "        ↓\n",
    "    MIN{2, a, b},\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 5:\n",
    "MAX{                  ⍺ = 3\n",
    "    MIN{3, 5, 10},\n",
    "           β = 2(pruning because MIN{2, a, b} <= 2 <= 3, result of outer MAX will never fall on MIN{2, a, b})\n",
    "           ↓\n",
    "    MIN{2, a, b},\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 6:\n",
    "MAX{                  ⍺ = 3\n",
    "    MIN{3, 5, 10},\n",
    "    MIN{2, a, b},\n",
    "        β = ∞\n",
    "        ↓\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 7:\n",
    "MAX{                  ⍺ = 3\n",
    "    MIN{3, 5, 10},\n",
    "    MIN{2, a, b},\n",
    "           β = 7\n",
    "           ↓\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "\n",
    "Step 8:\n",
    "MAX{                  ⍺ = 3\n",
    "    MIN{3, 5, 10},\n",
    "    MIN{2, a, b},\n",
    "              β = 2(pruning because MIN{7, 2, 3} <= 2 <= 3, result of outer MAX will never fall on MIN{7, 2, 3})\n",
    "              ↓\n",
    "    MIN{7, 2, 3},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Complexity_: ####\n",
    "\n",
    "   _Assmue each node has b successors and depth is d_<br>\n",
    "   _Worst Case_: No pruning happening, same complexity as minimax<br>\n",
    "   _Best Case_: First evaluated node is the best node, so that 1 for MAX layer and b for last MIN layer(it is possible to have multiple MIN layers)\n",
    "   \n",
    "             O(1ˣbˣ1ˣbˣ...ˣb) = O(b^(d/2))\n",
    "             \n",
    "    Therefore, within the same amount of time Minimax with alpha-beta pruning could traverse 2 times deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBetaAgent(MinimaxPolicy):\n",
    "    def __init__(self, index, depth):\n",
    "        \"\"\"Agent using Alpha-Beta Pruning algorithm\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Current agent index in agent array.\n",
    "        depth : int\n",
    "            The depth of game tree going to expand.\n",
    "        \"\"\"\n",
    "        self._player_optimizer = (self.maximizer, (-float('inf'), None))\n",
    "        self._opponent_optimizer = (self.minimizer, (float('inf'), None))\n",
    "\n",
    "        return super().__init__(index=index, depth=depth)\n",
    "    \n",
    "    def evaluationFunction(self, game_state):\n",
    "        \"\"\"        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_state : State\n",
    "            Game State.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Value associated with the game state\n",
    "        \"\"\" \n",
    "        game_state.get_score()\n",
    "    \n",
    "    def get_optimize_specifics(self, agent_index):\n",
    "        \"\"\"\n",
    "        Get optimizer and inital best score\n",
    "        \"\"\"\n",
    "        if agent_index == self.index:\n",
    "            return self._player_optimizer\n",
    "        else:\n",
    "            return self._opponent_optimizer\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        \"\"\"\n",
    "          Returns the action associated with best score\n",
    "        \"\"\"\n",
    "        _, action = self.minimax(self.index,\n",
    "                                 gameState,\n",
    "                                 1,\n",
    "                                 (-float('inf'), None),\n",
    "                                 (float('inf'), None))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectimax ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Description_: ####\n",
    "   - MAX layer remains the same\n",
    "   - MIN layer is replaced by chance nodes, that's to say each possible opponent move is associated with a weight(possibility) and the result is the sum of (weight ˣ score)\n",
    "   - Minimax almost could be considered as a special case of expectimax that min value has weight of 1.0 and others are 0\n",
    "```\n",
    "    Solves more realistic problem that result will not be too much biased by the minimum value since weights could be changed according to different kinds of opponents, so we don't need to assume opponent makes the optimal move.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Example_: ####\n",
    "![image info](./expectimax.jpg)\n",
    "\n",
    "```\n",
    "MAX{\n",
    "    w1ˣA + w2ˣB + w3ˣC,\n",
    "    w4ˣD + w5ˣE + w6ˣF,\n",
    "    w7ˣG + w8ˣH + w9ˣI,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectimaxAgent(MinimaxPolicy):\n",
    "    def __init__(self, index, depth):\n",
    "        \"\"\"Agent using Expectimax algorithm\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Current agent index in agent array.\n",
    "        depth : int\n",
    "            The depth of game tree going to expand.\n",
    "        \"\"\"\n",
    "        self._player_optimizer = (self.maximizer, (-float('inf'), None))\n",
    "        self._opponent_optimizer = (self.expectation_adder, (0, None))\n",
    "        \n",
    "        return super().__init__(index=index, depth=depth)\n",
    "    \n",
    "    def evaluationFunction(self, game_state):\n",
    "        \"\"\"        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_state : State\n",
    "            Game State.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Value associated with the game state\n",
    "        \"\"\" \n",
    "        game_state.get_score()\n",
    "    \n",
    "    def get_optimize_specifics(self, agent_index):\n",
    "        \"\"\"\n",
    "        Get optimizer for player or opponent\n",
    "        \"\"\"\n",
    "        if agent_index == self.index:\n",
    "            return self._player_optimizer\n",
    "        else:\n",
    "            return self._opponent_optimizer\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        \"\"\"\n",
    "        Returns the action associated with best score\n",
    "        \"\"\"\n",
    "        _, action = self.minimax(self.index,\n",
    "                                 gameState,\n",
    "                                 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zobrist Hash ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Decription_: #####\n",
    "   - Compute the hash of a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Steps_: #####\n",
    "   1. Initialize a 3 dimensional table that contains keys for each possible case on board\n",
    "   2. Start with 0 and do XOR operation for each non empty position on board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Advantages_: #####\n",
    "   1. When player makes a move, no need to recalculate everything because (A XOR B XOR B = A)\n",
    "   2. Less hash table collision (Complex mathmatical prove behind this, Skip)\n",
    "   \n",
    "##### _Disadvantages_: #####\n",
    "   1. Common drawback of tabulation hash that requires certain amount of memory to store keys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Example_: #####\n",
    "\n",
    "_Tic-Tac-Toe_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4473658442111030265\n",
      "6235174913376703316\n",
      "4473658442111030265\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# dictionary storing the piece keys to zobrist table\n",
    "pieces = {\n",
    "    'x': 0,\n",
    "    'o': 1,\n",
    "}\n",
    "board_height = 3\n",
    "board_width = 4\n",
    "# Zobrist table value for each piece in board\n",
    "zobrist_table = [[[randint(1, 2**63) for _ in pieces] for _ in range(board_width)] for _ in range(board_height)]\n",
    "\n",
    "def get_hash(board):\n",
    "    height = len(board)\n",
    "    width = len(board[0])\n",
    "    h_val = 0\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            piece = board[y][x]\n",
    "            if piece in pieces:\n",
    "                piece_key = pieces[piece]\n",
    "                h_val ^= zobrist_table[y][x][piece_key]\n",
    "    return h_val\n",
    "\n",
    "#@todo wrap this function in a class so that previous_board_hash == hash(previous_board) \n",
    "def update_hash(board, previous_board, previous_hash, positions):\n",
    "    new_hash = previous_hash\n",
    "    for position in positions:\n",
    "        y, x = position\n",
    "        \n",
    "        previous_piece = previous_board[y][x]\n",
    "        if previous_piece in pieces:\n",
    "            piece_key = pieces[previous_piece]\n",
    "            new_hash ^= zobrist_table[y][x][piece_key]\n",
    "            \n",
    "        current_piece = board[y][x]\n",
    "        if current_piece in pieces:\n",
    "            piece_key = pieces[current_piece]\n",
    "            new_hash ^= zobrist_table[y][x][piece_key]\n",
    "    return new_hash\n",
    "\n",
    "previous_board = [\n",
    "    ['x', 'o', '_', '_'],\n",
    "    ['_', 'x', '_', 'o'],\n",
    "    ['_', 'o', '_', 'x'],\n",
    "]\n",
    "board = [\n",
    "    ['x', 'o', 'o', '_'],\n",
    "    ['_', 'x', 'o', 'o'],\n",
    "    ['_', 'o', 'o', 'x'],\n",
    "] # updated ((0, 2), (1, 2), (2, 2))\n",
    "\n",
    "# previous hash\n",
    "previous_hash = get_hash(previous_board)\n",
    "print(previous_hash)\n",
    "\n",
    "# updated hash\n",
    "current_hash = update_hash(board, previous_board, previous_hash, ((0, 2), (1, 2), (2, 2)))\n",
    "print(current_hash)\n",
    "\n",
    "# Should get the same value as previous_hash\n",
    "print(update_hash(previous_board, board, current_hash, ((0, 2), (1, 2), (2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaton Function ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Decription_: #####\n",
    "\n",
    "   To get value of the state, depending on rules of the game, also there are some learnings about using Reinforcement Learning on evaluation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Exploration ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Example_: ####\n",
    "![img](http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopi/MonteCarloPiMod/Images/MonteCarloPiMod_gr_5.gif)\n",
    "\n",
    "    Sampleing randomly on the square area and will get the result that\n",
    "    \n",
    "    # of samples in circle\n",
    "    ---------------------- = π\n",
    "      # of total samples\n",
    "      \n",
    "    This can also be used in simulating odds for gambling games such as Texas Holdem Poker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "# Thanks #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
